vgm <- variogram(ndvi ~ 1, sp_data)
fit <- fit.variogram(vgm, model = vgm("Sph"))
grid <- expand.grid(
longitude = seq(min(data$longitude), max(data$longitude), length = 50),
latitude = seq(min(data$latitude), max(data$latitude), length = 50)
)
coordinates(grid) <- ~longitude+latitude
gridded(grid) <- TRUE
proj4string(grid) <- CRS("+proj=longlat +datum=WGS84")
# krigagem
kriged <- krige(ndvi ~ 1, sp_data, grid, model = fit)
return(list(variogram = vgm, fitted_model = fit, kriged_data = kriged))
}
plot_ndvi_results <- function(model_results, shapefile) {
kriged_df <- as.data.frame(model_results$kriged_data)
r <- raster(nrows = 50, ncols = 50)
extent(r) <- extent(min(kriged_df$longitude), max(kriged_df$longitude),
min(kriged_df$latitude), max(kriged_df$latitude))
r <- rasterize(kriged_df[, c("longitude", "latitude")], r, kriged_df$var1.pred)
crs(r) <- CRS("+proj=longlat +datum=WGS84")
shapefile_sf <- st_transform(shapefile, 4326)
r_masked <- mask(r, as(shapefile_sf, "Spatial"))
pal <- colorNumeric(
palette = colorRampPalette(c("#a50026", "#fdae61", "#d9ef8b", "#006837"))(100),
domain = values(r_masked),
na.color = "transparent"
)
m <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) %>%  # Better base map
addRasterImage(r_masked,
colors = pal,
opacity = 0.8) %>%
addPolygons(data = shapefile_sf,
weight = 2,
opacity = 1,
color = "#444444",
fillOpacity = 0,
dashArray = "3") %>%
addLegend(pal = pal,
values = values(r_masked),
title = "NDVI Values",
position = "bottomright",
labFormat = labelFormat(digits = 2)) %>%
addScaleBar(position = "bottomleft") %>%
addMiniMap(position = "bottomleft",
toggleDisplay = TRUE)
plot(model_results$variogram,
model_results$fitted_model,
main = "NDVI Variogram",
pch = 19)
return(m)
}
model <- create_ndvi_model(dados_completos)
validar_krigagem <- function(model) {
variogram_fit <- model$fitted_model
kriged_data <- model$kriged_data
predictions <- kriged_data$var1.pred
variance <- kriged_data$var1.var
nugget <- variogram_fit$psill[1]  # Efeito pepita
sill <- sum(variogram_fit$psill)  # Patamar total
range <- variogram_fit$range[2]   # Alcance
structural_variance <- (sill - nugget) / sill
mean_variance <- mean(variance, na.rm = TRUE)
sd_variance <- sd(variance, na.rm = TRUE)
results <- data.frame(
Metric = c("Nugget", "Sill", "Range", "Structural Variance Ratio",
"Mean Kriging Variance", "SD Kriging Variance"),
Value = c(nugget, sill, range, structural_variance,
mean_variance, sd_variance)
)
hist(variance, main = "Histogram of Kriging Variance",
xlab = "Variance", breaks = 30)
return(list(
metrics = results,
prediction_summary = summary(predictions),
variance_summary = summary(variance)
))
}
create_kriging_assessment <- function(metrics) {
classify_metric <- function(metric, value) {
if(metric == "Structural Variance Ratio") {
if(value > 0.75) return("Ótimo (>0.75)")
if(value > 0.50) return("Bom (0.50-0.75)")
if(value > 0.25) return("Regular (0.25-0.50)")
return("Ruim (<0.25)")
}
if(metric == "Nugget") {
if(value < 0.05) return("Ótimo (<0.05)")
if(value < 0.10) return("Bom (0.05-0.10)")
if(value < 0.15) return("Regular (0.10-0.15)")
return("Alto (>0.15)")
}
if(metric == "Mean Kriging Variance") {
if(value < 0.05) return("Ótimo (<0.05)")
if(value < 0.10) return("Bom (0.05-0.10)")
if(value < 0.15) return("Regular (0.10-0.15)")
return("Alto (>0.15)")
}
return("--")
}
# Criar dataframe com interpretações
assessment <- data.frame(
Metrica = c("Nugget (Efeito Pepita)",
"Structural Variance Ratio",
"Range (Alcance)",
"Mean Kriging Variance",
"SD Kriging Variance"),
Valor = c(metrics$Value[1],
metrics$Value[4],
metrics$Value[3],
metrics$Value[5],
metrics$Value[6]),
Classificacao = c(
classify_metric("Nugget", metrics$Value[1]),
classify_metric("Structural Variance Ratio", metrics$Value[4]),
"--",
classify_metric("Mean Kriging Variance", metrics$Value[5]),
"--"
),
Interpretacao = c(
"Indica o nível de ruído aleatório nos dados. Quanto menor, melhor.",
"Proporção da variância explicada pela estrutura espacial. Quanto maior, melhor.",
"Distância máxima de dependência espacial entre os pontos.",
"Média da incerteza nas predições. Quanto menor, melhor.",
"Variabilidade da incerteza nas predições. Quanto menor, mais homogênea a precisão."
)
)
return(assessment)
}
fast_validation <- validar_krigagem(model)
kable(fast_validation$metrics)
create_kriging_assessment_styled <- function(metrics) {
# Criar dataframe base
assessment <- data.frame(
Metrica = c(
"Efeito Pepita (Nugget)",
"Patamar (Sill)",
"Alcance (Range)",
"Razão de Variância Estrutural",
"Média da Variância de Krigagem",
"Desvio Padrão da Variância"
),
Valor = c(
format(metrics$Value[1], digits = 4),
format(metrics$Value[2], digits = 4),
format(metrics$Value[3], digits = 4),
paste0(format(metrics$Value[4] * 100, digits = 2), "%"),
format(metrics$Value[5], digits = 4),
format(metrics$Value[6], digits = 4)
)
)
# Criar tabela estilizada
styled_table <- assessment %>%
kable(format = "html",
align = "c",
col.names = c("Métrica", "Valor")) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = FALSE,
position = "center") %>%
column_spec(1, bold = TRUE, color = "white", background = "#4CAF50") %>%
column_spec(2, color = "#4CAF50", background = "#F0F0F0", width = "5cm") %>%
add_header_above(c("Avaliação da Krigagem" = 2))
return(styled_table)
}
# Função para criar uma segunda tabela com interpretações
create_kriging_interpretation_styled <- function(metrics) {
# Definir critérios de avaliação
structural_var_ratio <- metrics$Value[4]
nugget <- metrics$Value[1]
mean_variance <- metrics$Value[5]
interpretation <- data.frame(
Metrica = c(
"Qualidade do Modelo",
"Dependência Espacial",
"Confiabilidade das Predições"
),
Avaliacao = c(
ifelse(structural_var_ratio > 0.75, "Excelente",
ifelse(structural_var_ratio > 0.5, "Boa", "Regular")),
ifelse(nugget < 0.05, "Forte",
ifelse(nugget < 0.15, "Moderada", "Fraca")),
ifelse(mean_variance < 0.05, "Alta",
ifelse(mean_variance < 0.15, "Moderada", "Baixa"))
)
)
# Criar tabela estilizada
styled_table <- interpretation %>%
kable(format = "html",
align = "c",
col.names = c("Critério", "Avaliação")) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = FALSE,
position = "center") %>%
column_spec(1, bold = TRUE, color = "white", background = "#4CAF50") %>%
column_spec(2, color = "#4CAF50", background = "#F0F0F0", width = "5cm") %>%
add_header_above(c("Interpretação dos Resultados" = 2))
return(styled_table)
}
tabela_metricas <- create_kriging_assessment_styled(fast_validation$metrics)
tabela_interpretacao <- create_kriging_interpretation_styled(fast_validation$metrics)
tabela_metricas <- create_kriging_assessment_styled(fast_validation$metrics)
tabela_interpretacao <- create_kriging_interpretation_styled(fast_validation$metrics)
tabela_metricas
tabela_interpretacao
tabela_metricas <- create_kriging_assessment_styled(create_kriging_assessment_styled$metrics)
create_kriging_assessment_styled <- function(metrics) {
assessment <- data.frame(
Metrica = c(
"Efeito Pepita (Nugget)",
"Patamar (Sill)",
"Alcance (Range)",
"Razão de Variância Estrutural",
"Média da Variância de Krigagem",
"Desvio Padrão da Variância"
),
Valor = c(
format(metrics$Value[1], digits = 4),
format(metrics$Value[2], digits = 4),
format(metrics$Value[3], digits = 4),
paste0(format(metrics$Value[4] * 100, digits = 2), "%"),
format(metrics$Value[5], digits = 4),
format(metrics$Value[6], digits = 4)
)
)
# Criar tabela estilizada
styled_table <- assessment %>%
kable(format = "html",
align = "c",
col.names = c("Métrica", "Valor")) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = FALSE,
position = "center") %>%
column_spec(1, bold = TRUE, color = "white", background = "#4CAF50") %>%
column_spec(2, color = "#4CAF50", background = "#F0F0F0", width = "5cm") %>%
add_header_above(c("Avaliação da Krigagem" = 2))
return(styled_table)
}
create_kriging_interpretation_styled <- function(metrics) {
structural_var_ratio <- metrics$Value[4]
nugget <- metrics$Value[1]
mean_variance <- metrics$Value[5]
interpretation <- data.frame(
Metrica = c(
"Qualidade do Modelo",
"Dependência Espacial",
"Confiabilidade das Predições"
),
Avaliacao = c(
ifelse(structural_var_ratio > 0.75, "Excelente",
ifelse(structural_var_ratio > 0.5, "Boa", "Regular")),
ifelse(nugget < 0.05, "Forte",
ifelse(nugget < 0.15, "Moderada", "Fraca")),
ifelse(mean_variance < 0.05, "Alta",
ifelse(mean_variance < 0.15, "Moderada", "Baixa"))
)
)
styled_table <- interpretation %>%
kable(format = "html",
align = "c",
col.names = c("Critério", "Avaliação")) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = FALSE,
position = "center") %>%
column_spec(1, bold = TRUE, color = "white", background = "#4CAF50") %>%
column_spec(2, color = "#4CAF50", background = "#F0F0F0", width = "5cm") %>%
add_header_above(c("Interpretação dos Resultados" = 2))
return(styled_table)
}
fast_validation <- validate_kriging_fast(model)
tabela_metricas <- create_kriging_assessment_styled(fast_validation$metrics)
tabela_interpretacao <- create_kriging_interpretation_styled(fast_validation$metrics)
tabela_metricas
tabela_metricas
tabela_interpretacao
create_ndvi_sar <- function(data, shapefile) {
set.seed(sqrt(2))
points_sf <- st_as_sf(data, coords = c("longitude", "latitude"),
crs = 4326)
shapefile <- st_transform(shapefile, 4326)
bbox <- st_bbox(shapefile)
grid_size <- 0.005
grid <- st_make_grid(shapefile,
cellsize = c(grid_size, grid_size),
what = "polygons") %>%
st_sf() %>%
st_cast("POLYGON")
grid <- st_intersection(grid, shapefile) %>%
st_make_valid() %>%
st_cast("POLYGON")
grid <- grid[!st_is_empty(grid), ]
grid_buffered <- st_buffer(grid, dist = grid_size/50)
nb <- poly2nb(st_geometry(grid_buffered),
queen = TRUE,
snap = grid_size/5)
gc <- spdep::n.comp.nb(nb)
if (gc$nc > 1) {
cat("Encontrados", gc$nc, "componentes desconectados. Usando o maior componente.\n")
biggest <- which.max(table(gc$comp.id))
keep <- gc$comp.id == biggest
grid <- grid[keep,]
nb <- poly2nb(st_geometry(grid),
queen = TRUE,
snap = grid_size/5)
}
W <- nb2mat(nb, style = "B", zero.policy = TRUE)
point_grid_index <- st_intersects(points_sf, grid, sparse = FALSE)
if(all(!point_grid_index)) {
stop("No points were assigned to grid cells. Check spatial alignment.")
}
data$grid_id <- max.col(point_grid_index)
grid_data <- data %>%
group_by(grid_id) %>%
summarise(
ndvi_mean = mean(ndvi, na.rm = TRUE),
ndvi_sd = sd(ndvi, na.rm = TRUE),
n = n()
) %>%
filter(!is.na(ndvi_mean))
n_cells <- nrow(grid)
full_grid_data <- data.frame(
grid_id = 1:n_cells,
ndvi_mean = NA,
ndvi_sd = NA,
n = 0
)
full_grid_data[grid_data$grid_id, c("ndvi_mean", "ndvi_sd", "n")] <-
grid_data[, c("ndvi_mean", "ndvi_sd", "n")]
# Adicionar uma cópia do grid_id para o efeito SAR
full_grid_data$grid_id_sar <- full_grid_data$grid_id
# --- Definir a fórmula do modelo ---
# Usando f() para definir o efeito SAR
formula <- ndvi_mean ~ 1 +
f(grid_id, model = "besag", graph = W) +
f(grid_id_sar, model = "sar", graph = W, scale.model = TRUE, param = c(0, 0.01)) # Prior para rho
# --- Ajustar o modelo INLA ---
model <- try(inla(
formula,
family = "gaussian",
data = full_grid_data,
control.predictor = list(compute = TRUE),
control.compute = list(dic = TRUE, waic = TRUE),
control.inla = list(strategy = "simplified.laplace", int.strategy = "ccd", cmin = 0.5),
control.family = list(hyper = list(prec = list(initial = log(0.1), fixed = TRUE))) # Prior para a precisão dos resíduos
))
if (inherits(model, "try-error")) {
stop("O modelo INLA falhou. Cheque seus dados e estrutura espacial.")
}
# --- Retornar Resultados ---
return(list(
model = model,
grid = grid,
data = full_grid_data,
W = W
))
}
plot_sar_results <- function(sar_results, shapefile) {
grid <- sar_results$grid
grid$fitted <- sar_results$model$summary.fitted.values$mean
pal <- colorNumeric(
palette = colorRampPalette(c("#a50026", "#fdae61", "#d9ef8b", "#006837"))(100),
domain = grid$fitted,
na.color = "transparent"
)
m <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addPolygons(data = grid,
fillColor = ~pal(fitted),
fillOpacity = 0.8,
weight = 0.5,
color = "#444444") %>%
addPolygons(data = shapefile,
weight = 2,
opacity = 1,
color = "#444444",
fillOpacity = 0,
dashArray = "3") %>%
addLegend(pal = pal,
values = grid$fitted,
title = "NDVI Values (SAR)",  # Changed title to reflect SAR model
position = "bottomright",
labFormat = labelFormat(digits = 2)) %>%
addScaleBar(position = "bottomleft") %>%
addMiniMap(position = "bottomleft",
toggleDisplay = TRUE)
return(m)
}
sar_model <- create_ndvi_sar(dados_completos, poa_shape)
library(sf)
library(dplyr)
library(INLA)
library(spdep)
library(leaflet)
create_ndvi_sar <- function(data, shapefile) {
set.seed(sqrt(2))
points_sf <- st_as_sf(data, coords = c("longitude", "latitude"),
crs = 4326)
shapefile <- st_transform(shapefile, 4326)
bbox <- st_bbox(shapefile)
grid_size <- 0.005
grid <- st_make_grid(shapefile,
cellsize = c(grid_size, grid_size),
what = "polygons") %>%
st_sf() %>%
st_cast("POLYGON")
grid <- st_intersection(grid, shapefile) %>%
st_make_valid() %>%
st_cast("POLYGON")
grid <- grid[!st_is_empty(grid), ]
grid_buffered <- st_buffer(grid, dist = grid_size/50)
nb <- poly2nb(st_geometry(grid_buffered),
queen = TRUE,
snap = grid_size/5)
gc <- spdep::n.comp.nb(nb)
if (gc$nc > 1) {
cat("Encontrados", gc$nc, "componentes desconectados. Usando o maior componente.\n")
biggest <- which.max(table(gc$comp.id))
keep <- gc$comp.id == biggest
grid <- grid[keep,]
nb <- poly2nb(st_geometry(grid),
queen = TRUE,
snap = grid_size/5)
}
W <- nb2mat(nb, style = "B", zero.policy = TRUE)
point_grid_index <- st_intersects(points_sf, grid, sparse = FALSE)
if(all(!point_grid_index)) {
stop("No points were assigned to grid cells. Check spatial alignment.")
}
data$grid_id <- max.col(point_grid_index)
grid_data <- data %>%
group_by(grid_id) %>%
summarise(
ndvi_mean = mean(ndvi, na.rm = TRUE),
ndvi_sd = sd(ndvi, na.rm = TRUE),
n = n()
) %>%
filter(!is.na(ndvi_mean))
n_cells <- nrow(grid)
full_grid_data <- data.frame(
grid_id = 1:n_cells,
ndvi_mean = NA,
ndvi_sd = NA,
n = 0
)
full_grid_data[grid_data$grid_id, c("ndvi_mean", "ndvi_sd", "n")] <-
grid_data[, c("ndvi_mean", "ndvi_sd", "n")]
# Adicionar uma cópia do grid_id para o efeito SAR
full_grid_data$grid_id_sar <- full_grid_data$grid_id
# Definir a fórmula usando o modelo SAR nativo
formula <- ndvi_mean ~ 1 +
f(grid_id, model = "besag", graph = W,
scale.model = TRUE,
hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01)))) +
f(grid_id_sar, model = "sar", graph = W,
scale.model = TRUE,
hyper = list(rho = list(prior = "pc.cor1", param = c(0.5, 0.5))))
# Ajustar o modelo mantendo as configurações que funcionavam bem
model <- try(inla(
formula,
family = "gaussian",
data = full_grid_data,
control.predictor = list(compute = TRUE),
control.compute = list(dic = TRUE, waic = TRUE),
control.inla = list(strategy = "gaussian")
))
if(inherits(model, "try-error")) {
stop("O modelo INLA falhou. Cheque seus dados e estrutura espacial.")
}
return(list(
model = model,
grid = grid,
data = full_grid_data,
W = W
))
}
plot_sar_results <- function(sar_results, shapefile) {
grid <- sar_results$grid
grid$fitted <- sar_results$model$summary.fitted.values$mean
pal <- colorNumeric(
palette = colorRampPalette(c("#a50026", "#fdae61", "#d9ef8b", "#006837"))(100),
domain = grid$fitted,
na.color = "transparent"
)
m <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addPolygons(data = grid,
fillColor = ~pal(fitted),
fillOpacity = 0.8,
weight = 0.5,
color = "#444444") %>%
addPolygons(data = shapefile,
weight = 2,
opacity = 1,
color = "#444444",
fillOpacity = 0,
dashArray = "3") %>%
addLegend(pal = pal,
values = grid$fitted,
title = "NDVI Values (SAR)",  # Changed title to reflect SAR model
position = "bottomright",
labFormat = labelFormat(digits = 2)) %>%
addScaleBar(position = "bottomleft") %>%
addMiniMap(position = "bottomleft",
toggleDisplay = TRUE)
return(m)
}
# Exemplo de uso:
# Assumindo que você tenha seus dados e shapefile:
# results <- create_ndvi_sar(data, shapefile)
# map <- plot_sar_results(results, shapefile)
# map  # Para visualizar o mapa
results <- create_ndvi_sar(data, shapefile)
sar_model <- create_ndvi_sar(dados_completos, poa_shape)
